{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mealbox service review sentiment analysis\n",
    "## Part 1: Scraping Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary function from created module to scrape data\n",
    "import nbimporter\n",
    "from review_scraper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping Google reviews\n",
    "marley_spoon_google_scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping Consumer Affairs reviews\n",
    "blue_apron_consumeraffairs_scrape()\n",
    "hello_fresh_consumeraffairs_scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping Trustpilot reviews\n",
    "blue_apron_trustpilot_scrape()\n",
    "hello_fresh_trustpilot_scrape()\n",
    "marley_spoon_trustpilot_scrape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Cleaning Review Data / Labeling\n",
    "The reviews that have been scraped using the above functions have been merged and cleaned using Microsoft Excel. The result was a file containing over 15k reviews of Marley Spoon, Hello Fresh and Blue Apron. Blue Apron did not have enough reviews within the time range of this research (Q1, Q2, Q3 2020). Therefore, we limited this research to just include Marley Spoon and Hello Fresh. <br>\n",
    "\n",
    "Since sentiment analysis on mealbox services has not been performed yet (at least not published), we were unable to train our model on an existing and labeled data set. To train the review sentiment model, 600 observations have been manually labeled to positive (1) or negative (0). \n",
    "\n",
    "The final result was an Excel workbook containing three sheets:\n",
    "<ul>\n",
    "  <li>All reviews (unlabeled)</li>\n",
    "  <li>All labeled reviews (training set)</li>\n",
    "  <li>All unlabeled reviews (All reviews - All labeled reviews)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Train models\n",
    "\n",
    "The following models have been trained and reviewed for their accuracy:\n",
    "<ul>\n",
    "  <li>Random Forest Classifier</li>\n",
    "  <li>Decision Tree Classifier</li>\n",
    "  <li>Logistic Regression Classifier</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the three sheets as specified under part 2.\n",
    "import pandas as pd\n",
    "all_reviews_unlabeled = pd.read_excel('Unlabeled (A), Labeled (B) and A - B.xlsx', sheet_name = 'Unlabeled total (A)')\n",
    "all_reviews_labeled = pd.read_excel('Unlabeled (A), Labeled (B) and A - B.xlsx', sheet_name = 'Labeled total (B)')\n",
    "all_labeled_reviews = pd.read_excel('Unlabeled (A), Labeled (B) and A - B.xlsx', sheet_name = 'A-B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the necessary libraries\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier as rfc\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy import stats\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "#In order to interact with the data in a easier way, will transfer the data to a dataframe\n",
    "df_labelled = all_reviews_labeled[['Label', 'Review']]\n",
    "\n",
    "#Just in case I will check for null values, shouldn't be any removed them in excel\n",
    "print('Check for null values: ',df_labelled[df_labelled['Label'].notnull()])\n",
    "\n",
    "#Should shuffle the dataset for later since it is a good practice\n",
    "df_lablled_2 = df_labelled.copy()\n",
    "df_labelled_2 = df_labelled.sample(frac=1)\n",
    "\n",
    "#Vectorization: will use bag of words model\n",
    "vectorizer = TfidfVectorizer()\n",
    "bow = vectorizer.fit_transform(df_labelled_2['Review'])\n",
    "labels = df_labelled_2['Label']\n",
    "#to see how many unique words (features) we have\n",
    "print('Total number of unique features:', len(vectorizer.get_feature_names()))\n",
    "\n",
    "#rule of thumb, should try to have at least ten time as many samples as features\n",
    "#so, will try to reduce the number of features by removing words which appear infrequently (less than 0.5%)\n",
    "vectorizer_2 = TfidfVectorizer(min_df=15)\n",
    "bow_2 = vectorizer_2.fit_transform(df_labelled_2['Review'])\n",
    "labels_2 = df_labelled_2['Label']\n",
    "print('Total number of unique features:', len(vectorizer_2.get_feature_names()))\n",
    "#the result is 254, which is more acceptable\n",
    "\n",
    "#Data partition: train and test sets 2/3 to train, 1/3 to test\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_2, labels_2, test_size=0.33)\n",
    "\n",
    "#Classifier: Random Forest(collections of decision trees)\n",
    "classifier = rfc()\n",
    "classifier.fit(X_train,y_train)\n",
    "print('Classifier Model Score:',classifier.score(X_test,y_test))\n",
    "\n",
    "#Hyperparameter Optimization:maximizing performance\n",
    "classifier_2 = rfc()\n",
    "hyperparameters = {'n_estimators':stats.randint(10,300),'criterion':['gini','entropy'],\n",
    "    'min_samples_split':stats.randint(2,9),'bootstrap':[True,False]}\n",
    "random_search = RandomizedSearchCV(classifier_2, hyperparameters, n_iter=65, n_jobs=4)\n",
    "random_search.fit(bow_2, labels_2)\n",
    "optimized_classifier = random_search.best_estimator_\n",
    "optimized_classifier.fit(X_train,y_train)\n",
    "print('Classifier Optimized Model Score:', optimized_classifier.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to interact with the data in a easier way, will transfer the data to a dataframe\n",
    "df_labelled = all_reviews_labeled[['Label', 'Review']]\n",
    "\n",
    "#check for nulls within the dataframe\n",
    "print('Check for null values: ',df_labelled[df_labelled['Label'].notnull()])\n",
    "\n",
    "#randomize the date under the sample size\n",
    "#so we don't have under numerical order within the data frame\n",
    "df_lablled_2 = df_labelled.copy()\n",
    "df_labelled_2 = df_labelled.sample(frac=1)\n",
    "df_labelled_2\n",
    "\n",
    "#preprocessing the inputs\n",
    "#assign the data input for the label encoder\n",
    "from sklearn import preprocessing\n",
    "datainput = df_labelled_2\n",
    "\n",
    "#use the label enconder function to convert the sting reviews into floats for our model is readable\n",
    "# since the labels are already encoded, the function will not make a difference except for\n",
    "#the Reviews column\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le_Number=LabelEncoder()\n",
    "le_Review= LabelEncoder()\n",
    "datainput['Review_n']=le_Number.fit_transform(datainput['Review'])\n",
    "datainput.drop([\"Review\"],axis=1)\n",
    "\n",
    "#assign X and y for the traning data set\n",
    "#datainput is still the same randomized dataframe yet encoded so the Reviews can fit the tree model\n",
    "X=datainput['Review_n'].values\n",
    "y=datainput[[\"Label\"]].values\n",
    "\n",
    "#We use the KFold to perfrom a Cross Validation for the train and test data set\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5)\n",
    "kf.get_n_splits(X)\n",
    "kf.get_n_splits(y)\n",
    "\n",
    "#find the shapes and train the datasets\n",
    "# new update from the sklearn.model_selection is the same module for the sklearn.cross_validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# constructing the model and shape the arrays\n",
    "#use the Descion Tree module from the sklearn library\n",
    "from sklearn import tree\n",
    "model = tree.DecisionTreeClassifier()\n",
    "X_train=X_train.reshape(-1,1)\n",
    "y_train=y_train.reshape(-1,1) \n",
    "X_test=X_test.reshape(-1,1)\n",
    "y_test=y_test.reshape(-1,1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#test for accuracy for the model\n",
    "#max depth will determing the dept of the tree and the random_state is the random genrations assigned\n",
    "from sklearn import metrics\n",
    "clf = tree.DecisionTreeClassifier(max_depth = 2, \n",
    "                             random_state = 0)\n",
    "clf = clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"\\nDecisionTrees's Accuracy: \", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary libraries\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "\n",
    "#Read the data in a pandas data frame\n",
    "df_labelled = all_reviews_labeled[['Label', 'Review']]\n",
    "\n",
    "#Splitting the review and labeled values\n",
    "review_text = df_labelled['Review'].values\n",
    "sentiment_labels = df_labelled['Label'].values\n",
    "\n",
    "#Splitting the data in train and test sets\n",
    "reviews_train, reviews_test, y_train, y_test = train_test_split(review_text, sentiment_labels, test_size=0.2, random_state=1)\n",
    "\n",
    "#Tokenize and build a vocubalary of known words\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(reviews_train)\n",
    "\n",
    "#Setting the X train and X test with the vectorized review data\n",
    "X_train = vectorizer.transform(reviews_train)\n",
    "X_test = vectorizer.transform(reviews_test)\n",
    "\n",
    "#Set placeholder variable for number of folds\n",
    "kfolds = 5\n",
    "\n",
    "#Set the min and max alpha for the cross-validation to find it's optimal level between\n",
    "min_alpha = 0.01\n",
    "max_alpha = 100\n",
    "\n",
    "#Generate the inverse of the min and max alpha since this is a requirement for LogisticRegressionCV\n",
    "max_C = 1/min_alpha\n",
    "min_C = 1/max_alpha\n",
    "\n",
    "#Set the number of candidates\n",
    "n_candidates = 5000\n",
    "\n",
    "#Import numpy to be able to create the list below\n",
    "import numpy as np\n",
    "\n",
    "#Create list to store C points in\n",
    "C_list = list(np.linspace(min_C, max_C, num=n_candidates))\n",
    "\n",
    "#Run the logistic regression cross-validation with a scoring metric of roc_auc\n",
    "clf_optimal = LogisticRegressionCV(Cs=C_list, cv=kfolds, max_iter=400, random_state=1, n_jobs=-1).fit(X_train, y_train)\n",
    "\n",
    "#Retrieve the accuracy\n",
    "accuracy = clf_optimal.score(X_test, y_test)\n",
    "print('Optimal Logistic Regression Classifier Accuracy:',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression Classifier proves to be the most accurate model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Predicting the sentiment on the total data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize and build a vocubalary of known words\n",
    "new_reviews = all_labeled_reviews['Review'].values\n",
    "X_newData = vectorizer.transform(new_reviews)\n",
    "\n",
    "#Predict the sentiment labels\n",
    "new_predictions = clf_optimal.predict(X_newData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copying data frame to avoid changing the original data\n",
    "labeled_reviews = all_labeled_reviews.copy()\n",
    "\n",
    "#Keeping only the relevant data\n",
    "labeled_reviews = labeled_reviews[['Review', 'Company', 'Date Posted']]\n",
    "\n",
    "#Adding a new column containing the new predictions\n",
    "labeled_reviews['Label'] = new_predictions\n",
    "\n",
    "#Appending the data that was already labeled\n",
    "labeled_reviews = labeled_reviews.append(all_reviews_labeled[['Review', 'Company', 'Date Posted', 'Label']])\n",
    "\n",
    "#Filtering out data that does not fall within Q1, Q2 or Q3\n",
    "final = labeled_reviews.copy()\n",
    "\n",
    "labeled_reviews_1 = labeled_reviews[(labeled_reviews['Date Posted'] >= '2020-01-01') & (labeled_reviews['Date Posted'] <= '2020-09-30')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Analysis\n",
    "### Keyword Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list with the columns of the dataframe we are interested to keep for the analysis\n",
    "col_keep = ['Review', 'Label']\n",
    "data_final = labeled_reviews_1[col_keep]\n",
    "\n",
    "#Separated reviews into positive and negative based on their label and droped the label column since we don't need them anymore\n",
    "positive_reviews = data_final.loc[data_final['Label']==1].drop(columns='Label')\n",
    "negative_reviews = data_final.loc[data_final['Label']==0].drop(columns='Label')\n",
    "\n",
    "#User defined function to measure term frequency \n",
    "def getmostcommonwords(reviews, n_most_common, stopwords=None):\n",
    "    #Flatten review column into a list of words, and set each to lowercase\n",
    "    flattened_reviews = [i for rev in reviews for i in rev.lower().split()]\n",
    "    #remove punctuation from the reviews\n",
    "    flattened_reviews = [''.join(char for char in rev if char not in string.punctuation) for rev in flattened_reviews]\n",
    "    #remove stopwords if applicable\n",
    "    if stopwords:\n",
    "        flattened_reviews = [word for word in flattened_reviews if word not in stopwords]        \n",
    "    #remove any empty string that were created by this process\n",
    "    flattened_reviews = [rev for rev in flattened_reviews if rev]\n",
    "    return Counter(flattened_reviews).most_common(n_most_common)\n",
    "\n",
    "#Remove certain stopwords from the list of stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "remove_words = set(('no','nor','not',\"don't\",'don','aren',\"aren't\"\\\n",
    "                    \"doesn't\",'doesn', \"didn't\", 'didn','isn',\"isn't\"\\\n",
    "                    \"wasn't\",'wasn'))\n",
    "new_stop_words = stop_words.difference(remove_words)   \n",
    "\n",
    "#Peliminary analysis with the objective of getting the FT\n",
    "#Most frequent words in positive dataframe with stopwords\n",
    "print('Most Common Words from positive reviews with stopwords NOT MERGING NOT: ',getmostcommonwords(positive_reviews['Review'], 50))\n",
    "#Most frequent words in positive dataframe without stopwords   \n",
    "print('Most Common Words from positive reviews without stopwords NOT MERGING NOT: ',getmostcommonwords(positive_reviews['Review'], 50,new_stop_words))\n",
    "#Most frequent words in negative dataframe with stopwords\n",
    "print('Most Common Words from negative reviews with stopwords NOT MERGING NOT: ',getmostcommonwords(negative_reviews['Review'], 50))\n",
    "#Most frequent words in negative dataframe without stopwords   \n",
    "print('Most Common Words from negative reviews without stopwords NOT MERGING NOT: ',getmostcommonwords(negative_reviews['Review'], 50,new_stop_words))\n",
    "\n",
    "#Since we noticed that a lot of the negative characteristics of the kit meal service\n",
    "#were denotated using words like 'not', 'no', 'didn't, etc we used a \n",
    "#User defined function to concatenate certain words to the next,\n",
    "#this way we are making sure we are not missing important negations\n",
    "def negate(text):\n",
    "\n",
    "    negation = False\n",
    "    include = True\n",
    "    result = []\n",
    "    words = text.split()\n",
    "\n",
    "    for word in words:\n",
    "        if any(neg in word for neg in [\"not\", \"n't\", \"no\", \"didn't\",\"can't\",'didnt'\\\n",
    "                                       'nor',\"don't\",'don','aren',\"aren't\"\\\n",
    "                                       \"doesn't\",'doesn','isn',\"isn't\"\\\n",
    "                                       \"wasn't\",'wasn']):\n",
    "            include = False\n",
    "            \n",
    "        stripped = word.strip('-').lower()\n",
    "        negated = \"not\" + stripped if negation else stripped\n",
    "        if include:\n",
    "            result.append(negated)\n",
    "        negation = False\n",
    "        include  = True\n",
    "\n",
    "        if any(neg in word for neg in [\"not\", \"n't\", \"no\", \"didn't\",\"can't\",'didnt'\\\n",
    "                                       'nor',\"don't\",'don','aren',\"aren't\"\\\n",
    "                                       \"doesn't\",'doesn','isn',\"isn't\"\\\n",
    "                                       \"wasn't\",'wasn']):\n",
    "            negation = not negation\n",
    "            \n",
    "    \n",
    "    result_2 = ' '.join(result)\n",
    "    return result_2\n",
    "\n",
    "#changed the dataframe to a list in order to use the user defined function\n",
    "pos_rev_2 = positive_reviews['Review'].values.tolist()\n",
    "neg_rev_2 = negative_reviews['Review'].values.tolist()\n",
    "\n",
    "#Used map to create new positive and negative reviews, that include the not before\n",
    "#the words identified in the user defined function\n",
    "pos_rev_3 = list(map(negate,pos_rev_2))\n",
    "neg_rev_3 = list(map(negate,neg_rev_2))\n",
    "\n",
    "#Created two data frames, one with positive and one with negative reviews which includes the not nefore the words\n",
    "#identified in the user defined function\n",
    "df_labelled_positive = pd.DataFrame(data=pos_rev_3, columns=['Review'])\n",
    "df_labelled_negative = pd.DataFrame(data=neg_rev_3,columns=['Review'])\n",
    "\n",
    "#Most frequent words in positive dataframe with stopwords\n",
    "print('Most Common Words from positive reviews with stopwords: ',getmostcommonwords(df_labelled_positive['Review'], 75))\n",
    "#Most frequent words in positive dataframe without stopwords   \n",
    "print('Most Common Words from positive reviews without stopwords: ',getmostcommonwords(df_labelled_positive['Review'], 75,new_stop_words))\n",
    "#Most frequent words in negative dataframe with stopwords\n",
    "print('Most Common Words from negative reviews with stopwords: ',getmostcommonwords(df_labelled_negative['Review'], 75))\n",
    "#Most frequent words in negative dataframe without stopwords   \n",
    "print('Most Common Words from negative reviews without stopwords: ',getmostcommonwords(df_labelled_negative['Review'], 75,new_stop_words))\n",
    "\n",
    "#Next step was to calculate de TF-IDF for the positive reviews removing the stopwords and using negate function\n",
    "tfidfVectorizer_pos = TfidfVectorizer(stop_words = new_stop_words,use_idf=True)\n",
    "#Here I get the tfidf of each words\n",
    "tfidf = tfidfVectorizer_pos.fit_transform(pos_rev_3)\n",
    "#Sum the rows of the matrix to get the tfidf of each word across all the reviews\n",
    "sum_of_tfidf = tfidf.sum(axis=0)\n",
    "#Saved matrix to data frame and got the words associated witht the tfidf\n",
    "#and order them in according to the tfidf value\n",
    "df_tfidf_positive = pd.DataFrame(data = sum_of_tfidf.T,index=tfidfVectorizer_pos.get_feature_names(),columns=['TF-IDF']).sort_values('TF-IDF', ascending=False)\n",
    "#Saved the data frame to a csv file\n",
    "df_tfidf_positive.to_csv('TI-IDF-Positive.csv') \n",
    "\n",
    "\n",
    "#Calculate de TF-IDF for the negative reviews removing the stopwords and using negate function\n",
    "tfidfVectorizer_neg = TfidfVectorizer(stop_words = new_stop_words,use_idf=True)\n",
    "#Here I get the tfidf of each words\n",
    "tfidf_neg = tfidfVectorizer_neg.fit_transform(neg_rev_3)\n",
    "#Sum the rows of the matrix to get the tfidf of each word across all the reviews\n",
    "sum_of_tfidf_neg = tfidf_neg.sum(axis=0)\n",
    "#Saved matrix to data frame and got the words associated witht the tfidf\n",
    "#and order them in according to the tfidf value\n",
    "df_tfidf_negative = pd.DataFrame(data=sum_of_tfidf_neg.T,index=tfidfVectorizer_neg.get_feature_names(),columns=['TF-IDF']).sort_values('TF-IDF', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Financial Comparison (Hello Fresh, Blue Apron, Marley Spoon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Blue Apron\n",
    "apron_rev = pd.read_csv(\"Apron Rev.csv\")\n",
    "\n",
    "#Rename column \"Orders (in thousands)\" to \"orders\" column \n",
    "apron_rev = apron_rev.rename(columns ={'Orders (in thousands)':'orders'}) \n",
    "\n",
    "#Rename column \"Customers (in thousands)\" to \"customer\" column \n",
    "apron_rev = apron_rev.rename(columns ={'Customers (in thousands)':'customer'}) \n",
    "\n",
    "#Rename column \"Net Revenue (in millions)\" to \"rev\" column \n",
    "apron_rev = apron_rev.rename(columns ={'Net Revenue (in millions)':'rev'})\n",
    "\n",
    "###Marley Spoon\n",
    "marley_rev = pd.read_csv(\"Marley Rev.csv\")\n",
    "\n",
    "#Rename column \"Number of orders (in thousands)\" to \"orders\"\n",
    "marley_rev = marley_rev.rename(columns ={'Number of orders (in thousands)':'orders'}) \n",
    "\n",
    "#Rename column \"Active customer (in thousands)\" to \"customer\"\n",
    "marley_rev = marley_rev.rename(columns ={'Active customer (in thousands)':'customer'}) \n",
    "\n",
    "#Rename column \"Net Revenue (in millions)\" to \"rev\" column \n",
    "marley_rev = marley_rev.rename(columns ={'Net Revenue (in millions)':'rev'}) \n",
    "\n",
    "###Hello Fresh\n",
    "hello_rev = pd.read_csv(\"HF Rev.csv\")\n",
    "\n",
    "# Rename column \"Number of orders(in millions)\" to \"orders\"\n",
    "hello_rev = hello_rev.rename(columns ={'Number of orders(in millions)':'orders'}) \n",
    "\n",
    "#Rename column 'Active customers(in millions)' to 'customer'\n",
    "hello_rev = hello_rev.rename(columns = {'Active customers(in millions)': 'customer'})\n",
    "\n",
    "#Rename column \"Revenue (in MUSD)\" to \"rev\" column \n",
    "hello_rev = hello_rev.rename(columns ={'Revenue (in MUSD)':'rev'}) \n",
    "\n",
    "#convert value in column order from in millions to in thousands for comparison purpose\n",
    "hello_rev['orders'] = (hello_rev['orders'].astype(float)*1000)\n",
    "hello_rev\n",
    "\n",
    "#convert value in column customer from in millions to in thousands for comparison purpose\n",
    "hello_rev['customer'] = (hello_rev['customer'].astype(float)*1000)\n",
    "hello_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Creating a bar chart for comparison\n",
    "date = apron_rev['Date']\n",
    "x = np.arange(len(apron_rev['Date']))\n",
    "w = 0.3\n",
    "\n",
    "plt.bar(x-w, apron_rev['orders'].values, width = w, label = \"Blue Apron \")\n",
    "plt.bar(x, marley_rev['orders'].values, width = w, label = \"Marley Spoon\")\n",
    "plt.bar(x+w, hello_rev['orders'].values, width = w, label = \"Hello Fresh\")\n",
    "\n",
    "plt.title('Orders', loc='center') # add title\n",
    "plt.xticks(x, date)   #define x to be date\n",
    "plt.xlabel(\"Date\")   #add label for x\n",
    "plt.ylabel(\"Orders (in thousands)\")    #add label for y \n",
    "plt.tight_layout()\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2), fancybox=True, ncol=5)   # anotation box position\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = apron_rev['Date']\n",
    "x = np.arange(len(apron_rev['Date']))\n",
    "w = 0.3\n",
    "plt.bar(x-w, apron_rev['customer'].values, width = w, label = \"Blue Apron \")\n",
    "plt.bar(x, marley_rev['customer'].values, width = w, label = \"Marley Spoon\")\n",
    "plt.bar(x+w, hello_rev['customer'].values, width = w, label = \"Hello Fresh\")\n",
    "\n",
    "plt.title('Customers', loc='center') # add title\n",
    "plt.xticks(x, date)  #define x to be date\n",
    "plt.xlabel(\"Date\")   #add label for x\n",
    "plt.ylabel(\"Customers (in thousands)\")   #add label for y \n",
    "plt.tight_layout()\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2), fancybox=True, ncol=5)   # anotation box position\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = apron_rev['Date']\n",
    "x = np.arange(len(apron_rev['Date']))\n",
    "w = 0.3\n",
    "\n",
    "plt.bar(x-w, apron_rev['rev'].values, width = w, label = \"Blue Apron \")\n",
    "plt.bar(x, marley_rev['rev'].values, width = w, label = \"Marley Spoon\")\n",
    "plt.bar(x+w, hello_rev['rev'].values, width = w, label = \"Hello Fresh\")\n",
    "\n",
    "plt.title('Net Revenue', loc='center') # add title\n",
    "plt.xticks(x, date)    #define x to be date\n",
    "plt.xlabel(\"Date\")       #add label for x\n",
    "plt.ylabel(\"Net Revenue (in Millions)\")      #add label for y\n",
    "plt.tight_layout()\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2), fancybox=True, ncol=5)     # anotation box position\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Stock prices\n",
    "\n",
    "##Marley Spoon\n",
    "#read the HFG.DE.csv file\n",
    "marley = pd.read_csv(\"MMM.AX.csv\")\n",
    "\n",
    "#Rename Pandas columns to lower case\n",
    "marley = marley.rename(str.lower, axis = 'columns')\n",
    "\n",
    "#Rename Pandas columns to remove white space\n",
    "marley.columns = marley.columns.str.replace(' ', \"\")\n",
    "marley.head()\n",
    "\n",
    "#Convert the format on Date column to YYYY-MM\n",
    "marley['date'] = pd.to_datetime(marley['date'].astype(str), format = '%Y-%m')\n",
    "\n",
    "#Create Plot line chart for stock price\n",
    "fig, ax = plt.subplots(figsize = (10,5))\n",
    "\n",
    "#add x-axis and y-axis\n",
    "ax.plot(marley['date'], marley['close'], color = 'green')\n",
    "\n",
    "#Set title and labels for axes\n",
    "ax.set(xlabel = \"Date\", \n",
    "      ylabel = \"Closing Price\", \n",
    "      title = \"Marley Spoon Price Chart (in AUD)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Hello Fresh\n",
    "#read the HFG.DE.csv file\n",
    "hellofresh = pd.read_csv(\"HFG.DE.csv\")\n",
    "\n",
    "#Rename Pandas columns to lower case\n",
    "hellofresh = hellofresh.rename(str.lower, axis = 'columns')\n",
    "\n",
    "#Rename Pandas columns to remove white space\n",
    "hellofresh.columns = hellofresh.columns.str.replace(' ', \"\")\n",
    "hellofresh.head()\n",
    "\n",
    "#Convert the format on Date column to YYYY-MM\n",
    "hellofresh['date'] = pd.to_datetime(hellofresh['date'].astype(str), format = '%Y-%m')\n",
    "\n",
    "#Create Plot line chart for stock price\n",
    "fig, ax = plt.subplots(figsize = (10,5))\n",
    "\n",
    "#add x-axis and y-axis\n",
    "ax.plot(hellofresh['date'], hellofresh['close'], color = 'green')\n",
    "\n",
    "#Set title and labels for axes\n",
    "ax.set(xlabel = \"Date\", \n",
    "      ylabel = \"Closing Price\", \n",
    "      title = \"Hello Fresh Price Chart (in EUR)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Blue Apron\n",
    "#read the APRN.csv file\n",
    "apron = pd.read_csv(\"APRN.csv\")\n",
    "\n",
    "#Rename Pandas columns to lower case\n",
    "apron = apron.rename(str.lower, axis = 'columns')\n",
    "\n",
    "#Rename Pandas columns to remove white space\n",
    "apron.columns = apron.columns.str.replace(' ', \"\")\n",
    "apron.head()\n",
    "\n",
    "#Convert the format on Date column to YYYY-MM\n",
    "apron['date'] = pd.to_datetime(apron['date'].astype(str), format = '%Y-%m')\n",
    "\n",
    "#Create Plot line chart for stock price\n",
    "fig, ax = plt.subplots(figsize = (10,5))\n",
    "\n",
    "#add x-axis and y-axis\n",
    "ax.plot(apron['date'], apron['close'], color = 'green')\n",
    "\n",
    "#Set title and labels for axes\n",
    "ax.set(xlabel = \"Date\", \n",
    "      ylabel = \"Closing Price\", \n",
    "      title = \"Blue Apron Price Chart (in USD)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Financials vs. Sentiment Relationship Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "pd.set_option('display.precision', 2)\n",
    "import re\n",
    "\n",
    "data = labeled_reviews_1.copy()\n",
    "\n",
    "data['week'] = data['Date Posted'].dt.week\n",
    "data['quarters'] = data['Date Posted'].dt.quarter\n",
    "data['month'] = data['Date Posted'].dt.month\n",
    "\n",
    "marley_df = data[data['Company'] == 'Marley Spoon']\n",
    "hello_df = data[data['Company'] == 'Hello Fresh']\n",
    "\n",
    "#Creating a percentage of total Crosstab for Marley Spoon\n",
    "marley_spoon_crossTab_week = pd.crosstab(marley_df['week'], marley_df[\"Label\"]).apply(lambda r: r/r.sum(), axis=1)\n",
    " \n",
    "#Creating a percentage of total Crosstab for Hello Fresh\n",
    "hello_fresh_crossTab_week = pd.crosstab(hello_df['week'], hello_df[\"Label\"]).apply(lambda r: r/r.sum(), axis=1)\n",
    "\n",
    "### Weekly and monthly for Marley Spoon\n",
    "\n",
    "#Creating a percentage of total Crosstab for the plot below\n",
    "marley_spoon_crossTab_quarterly = pd.crosstab(marley_df['quarters'], marley_df[\"Label\"]).apply(lambda r: r/r.sum(), axis=1)\n",
    "print(marley_spoon_crossTab_quarterly)\n",
    "\n",
    "marley = pd.read_csv(\"Marley Rev.csv\")\n",
    "marley\n",
    "\n",
    "#Rename column \"Number of orders (in thousands)\" to \"orders\"\n",
    "marley = marley.rename(columns ={'Number of orders (in thousands)':'orders'}) \n",
    "\n",
    "#Rename column \"Active customer (in thousands)\" to \"customer\"\n",
    "marley = marley.rename(columns ={'Active customer (in thousands)':'customer'}) \n",
    "\n",
    "#Rename column \"Net Revenue (in millions)\" to \"rev\" column \n",
    "marley = marley.rename(columns ={'Net Revenue (in millions)':'rev'}) \n",
    "\n",
    "### Weekly and monthly for Hello Fresh\n",
    "#Creating a percentage of total Crosstab for the plot below\n",
    "hello_crossTab_quarterly = pd.crosstab(hello_df['quarters'], hello_df[\"Label\"]).apply(lambda r: r/r.sum(), axis=1)\n",
    "\n",
    "print(hello_crossTab_quarterly)\n",
    "\n",
    "hello_rev = pd.read_csv(\"HF Rev.csv\")\n",
    "hello_rev\n",
    "\n",
    "# Rename column \"Number of orders(in millions)\" to \"orders\"\n",
    "hello_rev = hello_rev.rename(columns ={'Number of orders(in millions)':'orders'}) \n",
    "\n",
    "#Rename column 'Active customers(in millions)' to 'customer'\n",
    "hello_rev = hello_rev.rename(columns = {'Active customers(in millions)': 'customer'})\n",
    "\n",
    "#Rename column \"Revenue (in MUSD)\" to \"rev\" column \n",
    "hello_rev = hello_rev.rename(columns ={'Revenue (in MUSD)':'rev'}) \n",
    "\n",
    "#convert value in column order from in millions to in thousands for comparison purpose\n",
    "hello_rev['orders'] = (hello_rev['orders'].astype(float)*1000)\n",
    "hello_rev\n",
    "\n",
    "#convert value in column customer from in millions to in thousands for comparison purpose\n",
    "hello_rev['customer'] = (hello_rev['customer'].astype(float)*1000)\n",
    "hello_rev\n",
    "\n",
    "###Create comparison chart for Hello Fresh and Marley Spoon\n",
    "fig = plt.figure()\n",
    "ax3 = marley_spoon_crossTab_quarterly.plot(kind = \"bar\", figsize = (10,8), stacked = True)\n",
    "ax4 = ax3.twinx()\n",
    "ax4.plot(marley['Date'], marley['rev'], color = 'purple', label = \"Revenue\")\n",
    "ax4.legend('Revenue')\n",
    "ax3.set_title('Review and Revenue Changes in 9 months - Marley Spoon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine Revenue chart with label chart:\n",
    "fig = plt.figure()\n",
    "ax3 = hello_crossTab_quarterly.plot(kind = \"bar\", figsize = (10,8), stacked = True)\n",
    "ax4 = ax3.twinx()\n",
    "ax4.plot(hello_rev['Date'], hello_rev['rev'], color = 'purple', label = \"Revenue\")\n",
    "ax4.legend('Revenue')\n",
    "ax3.set_title('Review and Revenue Changes in 9 months - Hello Fresh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax3 = hello_crossTab_quarterly.plot(kind = \"bar\", figsize = (10,8), stacked = True)\n",
    "ax4 = ax3.twinx()\n",
    "ax4.plot(hello_rev['Date'], hello_rev['orders'], color = 'purple', label = \"Orders\")\n",
    "ax4.legend('Order')\n",
    "ax3.set_title('Review and Orders Changes in 9 months - Hello Fresh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax3 = marley_spoon_crossTab_quarterly.plot(kind = \"bar\", figsize = (10,8), stacked = True)\n",
    "ax4 = ax3.twinx()\n",
    "ax4.plot(marley['Date'], marley['orders'], color = 'purple', label = \"Orders\")\n",
    "ax4.legend('Order')\n",
    "ax3.set_title('Review and Orders Changes in 9 months - Marley Spoon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax3 = hello_crossTab_quarterly.plot(kind = \"bar\", figsize = (10,8), stacked = True)\n",
    "ax4 = ax3.twinx()\n",
    "ax4.plot(hello_rev['Date'], hello_rev['customer'], color = 'purple', label = \"Customers\")\n",
    "ax4.legend('Customers')\n",
    "ax3.set_title('Review and Customers Changes in 9 months - Hello Fresh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax3 = marley_spoon_crossTab_quarterly.plot(kind = \"bar\", figsize = (10,8), stacked = True)\n",
    "ax4 = ax3.twinx()\n",
    "ax4.plot(marley['Date'], marley['customer'], color = 'purple', label = \"Customers\")\n",
    "ax4.legend('Customers')\n",
    "ax3.set_title('Review and Customers Changes in 9 months - Marley Spoon')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
